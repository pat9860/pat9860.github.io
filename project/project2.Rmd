---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: '2020-12-02'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```
<P style="page-break-before: always">
\newpage


**0. (5 pts)** Introduction

  While not the biggest fan of trading sports cards myself, I definitely come from a family of people that were avid collectors of baseball, football, and basketball cards. The money and fanatacism behind these cards is incredible and can be very lucrative for some owners. This dataset decided to measure if the value of these collectible cards increased due to ownership. This was done by randomly selecting 148 total traders at a trading card show. Some of the variables are age of the participant (age), whether the trader was a card dealer or not (dealer), the number of trades per month done by the trader (trades_pm), the number of years that the trader had been involved in trading (years), which income group the trader fell in based on 8 different levels (income), what gender the trader was(gender), and their highest level of education based on 6 different levels (education). This particular study had randomly given its participants one of two different collectables, either good A or good B (shown in the variable "good"). The variable "trade"" measures whether or not the trader decided to  trade the good given to them for the other one. This dataset was derived from Stock and Watson (2007). 
```{R}
SportsCards <- read.csv("SportsCards.csv")
head(SportsCards)
colnames(SportsCards) <- c("participant", "good", "dealer", "trades_pm", "years", "income", "gender", "education", "age", "trade")
head(SportsCards)
glimpse(SportsCards)
``` 
**1. (15 pts)** MANOVA

  A one-way MANOVA was conducted to see the impacts of different levels of education on 3 numeric variables (age, trades made per month, and years trading). Our null hypothesis was that there was no difference between the mean of these variables based on education level, with our alternative stating that there was a difference.  An mshapiro test was done to assess the multivariate homogeneity, under which the assumption was violated. Due to this violation, no examination of covariance matrices for each group was conducted. No univariate or multivariate outliers were evident, and running a MANOVA was considered an appropriate test to run for analysis. 
  Significant differences were found among the 6 different levels of education for at least one of the numeric variables, Pillai trace = 0.24, pseudo F = 2.48, p = 0.00167.
  Univariate ANOVAs for each variable were then ran to follow up the results of the MANOVA, using the Bonferroni method to control Type 1 error rates used for these comparisons. Here we found that the univariate ANOVA for age was significant, F = 6.67, p<.0001. No type 1 error probability was calculated due to adjusted the significance level. 
  Post hoc analysis was then done to check pairwise comparisons to see which of the education levels differed in their mean age. After adjusting for multiple comparisons (bonferroni alpha = 0.05/19 = .003), only those with no more than an 8th grade level of education differed significantly in terms of age. 19 tests were conducted in total, including 1 MANOVA, 3 ANOVAs, and 15 t tests.
```{R}
#H0: For trades made per month, number of years trading, and age of the trader, there is no difference based on education. 
#HA: For at least one of the numeric variables, at least one level of educations mean is different. 

man1 <- manova(cbind(trades_pm, years, age)~education, data = SportsCards)
#MANOVA
summary(man1)
#is significant - have to run univariate ANOVA and post-hoc t test

#univariate ANOVAs from MANOVA object 
summary.aov(man1) 

#for age, at least one education level is different 

SportsCards %>% 
  group_by(education) %>%
  summarize(mean(age))

#postHOC t-test for age
pairwise.t.test(SportsCards$age,
                SportsCards$education, p.adj = "none")

#did 1 MANOVA, 3 ANOVAs, and 15 t tests (10 tests)
.05/19
#0.003 adjusted significance level
library(rstatix)
#assessing assumptions 
education<- SportsCards$education
DVs<- SportsCards %>% select(trades_pm, years)
#test multivariate normality for each group
sapply(split(DVs, education), mshapiro_test)
#assumption is violated 

```
**2. (10 pts)** Randomization Test

 We decided to run a randomization test looking at the mean difference in the number of years trading between men and women. My null hypothesis was that the mean number of years trading was the same for both genders, while my alternative hypothesis was that the mean number of years trading was different between the genders. 
  We found a test statistic of 3.845, which in our context meannt that males had 3.845 more years trading on average than females. This was our observed difference that was used as our test statistic. A Monte Carlo sample was taken for 5000 random permutations. Taking the mean of the vector used to store these randomization, I was unable to reject the null hypothesis. Our probability of getting a mean difference this extreme if there really was not a difference between the genders and the mean years they've been trading was 0.084, meaning that we fail reject the null. This evidence suggests that there was no difference in the mean number of years trading between men and women. A t-test was also ran to confirm this assumptiont (t= -2.9259, df = 29.63, p-value = 0.006532). Below is as plot of the null distribution and the test statistic. 

 
```{R}
#H0: mean number of years trading is the same for men as it is for women. 
#HA: mean number of years trading is diffrent for men and women. 

ggplot(SportsCards, aes(years, fill = gender)) + geom_histogram(bins = 5) +
  facet_wrap(~gender, ncol = 2)

SportsCards %>%
  group_by(gender) %>%
  summarize(mean_years = mean(years)) %>%
  summarize(diff(mean_years))

rand_dist <- vector()

for(i in 1:5000){
  new <- data.frame(years = sample(SportsCards$years), gender = SportsCards$gender)
  rand_dist[i] <- mean(new[new$gender =="male",]$years) - mean(new[new$gender == "female",]$years)
}

mean(rand_dist)

{hist(rand_dist, main = "Years Trading for Each Gender", ylab = ""); abline(v = c(-3.845, 3.845), col = "green"); }

#pvalue: fail to reject the null 
mean(rand_dist>3.845 | rand_dist < -3.845) 

#independent-samples t test for comparison
t.test(data = SportsCards, years~gender)
```
**3. (35 pts)** Linear Regression Model
    
  We decided to test if the effect of age on trades made per month differed depending whether or not the participant was a card dealer. Our null hypothesis was that there was no difference in the effect that age would have on trades per month for dealers and non-dealers. Our alternative hypthesis was that there was a difference. We mean centered the age of the participants at the card trading show. The intercept of 5.665 was the mean/predicted trades per month made for non-dealers of average age. For people of average age, dealers had an average/predicted trades per month that was 9.161 times greater than for non-dealers. (b = 9.161, t = 6.306, p = 3.30e-09). Age was associated with trades per month, as for every 1 unit increase in age, the predicted trades per month went down by 0.193. The slope of age on trades per month for card dealers was 0.297 greater than for non-dealers. Below is a plot of this regression, with the vertical lines being 0(dashed) and the mean age (solid). The horizontal lines correspond to whether the participant was a dealer or not. 
  The assumptions for linearity, homoskedasticity, and normality were assessed using hypothesis testing and graphing. Homoskedasticity looked fine for the model, which was assessed using the Breusch-Pagan test (BP = 5.2816, df = 3, p-value = 0.1523). According to the residuals graph, linearity also looked good. The model did not pass normality, however. Using the Wilke-Shapiro test, our model was found to be non-normal (W = 0.85829, p-value = 1.28e-10). Due to being non-normal, boot-strapped standard errors would need to be calculated to adjust for this. 
  Although our model did not violate the homoskedasticity assumption, we still calculated robust standard errors. After adjusting the standard errors, I did not observe much change between the corrected and non-corrected standard errors, except for the intercept, which decreased. This meant that there was a significant decrease in variation. All of our values are significant, even after making the standard errors robust to violations of homoskedasticity. Using the adjusted R^2 value, 23.47% of the variation in the outcome is explained by our model. This value was used instead of the regular R^2 because it had penalties for each of my explanatory variables. Even after adjustment, there was not much difference between the adjusted and non-adjusted R^2 value. 
  
```{R}
# Does the effect of age on trades per month differ whether or not you're a dealer? 
SportsCards$age_c <- SportsCards$age - mean(SportsCards$age)

fit <- lm(trades_pm ~ dealer*age_c, data = SportsCards)
summary(fit)

ggplot(SportsCards, aes(age_c, trades_pm, color = dealer)) +
  geom_smooth(method = "lm", se = F, fullrange = T) +
  geom_point() +
  geom_vline(xintercept = 0, lty = 2) +
  geom_vline(xintercept = mean(SportsCards$age))

anova(fit) 

library(lmtest)
library(sandwich)
#testing assumptions
resids <- fit$residuals
fitvals <- fit$fitted.values

#homoskedsaticity and linearity 
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red') 

par(mfrow=c(1,2)); hist(resids); qqnorm(resids); qqline(resids, col='red')

#test normality (non-normal)
shapiro.test(resids)

#test homoskedasticity (passes assumption)
bptest(fit)

#corrected SE
coeftest(fit, vcov. = vcovHC(fit))[,1:2]
#uncorrected SE
summary(fit)$coef[,1:2]

coeftest(fit, vcov = vcovHC(fit, type="HC1"))
```
**4. (5 pts)** Bootstrapping Standard Errors

  Taking a bootstrapped sample meant that we randomly sampled rows from the dataset with replacment. Due to our model failing the normality assumption, these were the standard errors we would use in order to not violate this assumption. Our bootstrapped standard errors were similar in value to our original and robust standard errors, more so to the latter. This meant that any changes in p-values would not be drastically different from the ones that were calculated with the robust standard errors.  
```{R fig.width=8}
boot_data <- sample_frac(SportsCards, replace = T)

samp_distn <- replicate(5000, {
  boot_data <- sample_frac(SportsCards, replace = T)
  fit2 <- lm(trades_pm ~ dealer*age_c, data = boot_data)
  coef(fit2)
})
samp_distn %>% t %>% as.data.frame %>%summarize_all(sd)

samp_distn %>% t %>% as.data.frame %>%summarize_all(mean)



```  
**5. (25 pts)** Logistic Regression Model
    
  I created a binary categorical variable (y) by converting whether or not the participant made the trade for the rare sports item, with "yes" being equal to 1. Controlling for years trading, there is not a significant difference between 2 years of college and any of the other education levels. Controlling for years trading, graduate school has an odds of making the trade that is 0.459 times that of 2 years of college. 4 years of college has an odds of making the trade that is 1.198 times that of 2 year colleges. High school education has an odds of making the trade that is 0.407 times that 2 year college. 8th grade has an odds of making the trade that is 1.456 times that 2 year college. Having some post-high school has an odds of making the trade that is 0.797 times that of 2 year college. Controlling for education level, for every 1 unit increase in years trading, odds of making the trade for the sports memorabilia changes by a factor of e^-.006 = 0.994 (they increase by 99.4%). This is not statistically significant. Controlling for both variables, our log odds of making the trade are 0.711 (71.1%). 
  A confusion matrix is located below in the coding, along with the density plot of the log odds. 
  The probability of predicting yes for a participant that actually made the trade is 0.02 (TPR). The probability of predicting no for a participant that did not make the trade is 0.97 (TNR). The proportion of people classified as making the trade that actually did is 0.25 (PPV). The accuracy of this model is 0.65. The AUC of the model when calculated using the class_diags function was 0.62, which means that our model did a poor job of quantifying how well we were predicting. When the AUC was calculated from the ROC curve, we did get a different value, this time of 0.77. This means that the ROC curve that was calculated did a fair, almost good job at properly quantifying its prediction of who did and did not make the trade for the rare item. 
```{R fig.width=8}
class_diag <- function(probs,truth){
#CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
f1=2*(sens*ppv)/(sens+ppv)
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,f1,auc) 
}

SportsCards <- SportsCards %>%
  mutate(y = ifelse(trade == "yes", 1,0))
head(SportsCards)

logit<-function(p)log(odds(p))

fit3 <- glm(y~education+years, data = SportsCards, family = binomial)
coeftest(fit3)

exp(coef(fit3))


newdat <- data.frame(years = rep(1:20,1), education = rep(c("8th grade", "high school", "2y college", "post-high school", "4y college", "graduate school"), each = 10))

newdat <- cbind(newdat, predict(fit3, newdat, se = T, type = "response"))

ggplot(newdat, aes(years, fit, color = education)) +
  geom_line() +
  ylab("prob") +
  geom_ribbon(aes(ymin=fit-se.fit,ymax=fit+se.fit), alpha=.3)
  
  
probs <- predict(fit3, type = "response")


table(predict = as.numeric(probs>.5), truth = SportsCards$y) %>%
  addmargins
  
class_diag(probs, SportsCards$y)

SportsCards$logit <- predict(fit3,type="link")  
    
SportsCards%>%
  ggplot()+geom_density(aes(logit,color=trade,fill=trade), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=-0.75)+xlab("logit (log-odds)")+
  geom_rug(aes(logit,color=trade))

sens<-function(p,data=data, y=y) mean(SportsCards[SportsCards$y==1,]$probs>p)
spec<-function(p,data=data, y=y) mean(SportsCards[SportsCards$y==0,]$probs<p)
sensitivity<-sapply(seq(0,1,.01),sens,SportsCards)
specificity<-sapply(seq(0,1,.01),spec,SportsCards)

ROC1<-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))
ROC1$TPR <- sensitivity
ROC1$FPR <- 1 - specificity

library(plotROC)

ROCplot<-ggplot(SportsCards)+geom_roc(aes(d=y,m=probs), n.cuts=0) 
ROCplot

calc_auc(ROCplot)
``` 
**6. (25 pts)** K-Fold CV and LASSO

  We chose to predict the outcome of making the trade given all of the variables, not with their interaction however.  
The accuracy of this model was 0.72, which is the proportion of the model actually being able to measure what it is meant to be measuring. The probability of predicting yes for a participant that actually made the trade is 0.44 (TPR). The probability of predicting no for someone who did not actually make the trade is 0.87 (TNR). The proportion of people classified as having made the trade that actually did is 0.64 (PPV). The AUC of the model is 0.77, which means that our model did a fair job at quantifying how well we were predicting. 
  This performance was overall worse for the out of sample model compared to the in sample model. Our accuracy was 0.71, out TPR was 0.36, and our TNR was 0.89. Most notably, this model was unable to calculate the precision of the model. That means that the proportion of people classified as having made the trade that actually did cannot be calculated using this data. Our AUC increased slightly, almost enough to change the classification from fair to good. This means that this model did a fair job of quantifying how well the model was predicting, despite overall performing worse than the in sample model.  
  After having performed LASSO on my model, the variables that were retained were participant number and trades per month. These were considered the most predictive variables. I do not think that what number participant a trader was would have a large impact on whether or not they made the trade, but I can understand why trades per month would. If someone has a proclivity towards making trades for other cards, that may influence their decision to trade cards for sports memorabilia. 
  The model using the LASSO variables performed worse than the logistic regression in the sample model. Our accuracy was 0.64, our TPR was 0.19, our TNR was 0.86, and this model actually calculated a precision which was 0.43. When comparing the two AUCs, we see a jump down from an estimated 0.77 to 0.70, which would mean that this model performed a worse job of quantifying how well we were predicting these outcomes. 
```{R fig.width=8}
library(glmnet)
fit4 <- glm(y~ participant + good + dealer + trades_pm + years + income + gender + education + age, data = SportsCards, family = "binomial")    

summary(fit4)
    
exp(coef(fit4))

probs2 <- predict(fit4, type = "response")

class_diag(probs2, SportsCards$y)

  
ROCplot2 <- ggplot(SportsCards) + 
  geom_roc(aes(d = y, m = probs2), n.cuts = 0)

calc_auc(ROCplot2)


set.seed(1234)
k = 10
cvdata <- SportsCards[sample(nrow(SportsCards)),]
cvfolds <- cut(seq(1:nrow(SportsCards)), breaks = k, labels = F)

diags <- NULL
for(i in 1:k){
  train <- cvdata[cvfolds!=i,]
  test <- cvdata[cvfolds==i,]
  truth <- test$y
  fit5 <- glm(y~ good + dealer + trades_pm + years + income + gender + education + age, data = SportsCards, family = "binomial") 
  probs2 <- predict(fit5, newdata = test, type = "response")
  diags <- rbind(diags, class_diag(probs2, truth))
}

summarize_all(diags, mean)

class_diag(probs, SportsCards$y)

y <- as.matrix(SportsCards$trade)
x <- model.matrix(y~ participant + good + dealer + trades_pm + years + income + gender + education + age, data = SportsCards)[,-1]

set.seed(1234)

x<- scale(x)
cv <- cv.glmnet(x,y, family = "binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)
  
diags2 <- NULL

for(i in 1:k){
  train <- cvdata[cvfolds!=i,]
  test <- cvdata[cvfolds == i,]
  truth <- test$y
  fit6 <- glm(y~participant+trades_pm, data = train, family = "binomial")
  probs3 <- predict(fit6, newdata = test, type = "response")
  diags2 <- rbind(diags2, class_diag(probs3, truth))
}

diags2 %>%
  summarize_all(mean)
  
```

```{R eval=F}
data(package = .packages(all.available = TRUE))
```

...
